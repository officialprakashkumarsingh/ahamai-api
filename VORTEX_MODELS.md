# ğŸŒªï¸ VORTEX Models Integration

## Overview
**18 new models** from VORTEX endpoint with **100% success rate**!

**Endpoint**: `http://141.11.210.85:3000/api/openai/v1`  
**Authentication**: None required âœ…  
**Test Date**: August 23, 2025  
**Success Rate**: 100% (18/18 working)

## âš¡ Performance Highlights

### ğŸ† Top 5 Fastest Models
1. **vortex-llama-guard-4-12b** - 189ms ğŸ¥‡
2. **vortex-gemma2-9b-it** - 198ms ğŸ¥ˆ
3. **vortex-llama3-70b-8192** - 220ms ğŸ¥‰
4. **vortex-llama-4-scout-17b-16e** - 355ms
5. **vortex-llama-3.3-70b-versatile** - 373ms

All VORTEX models are **extremely fast**, with 16 out of 18 models responding in under 1 second!

## âœ… All Working Models

### Lightning Fast (<400ms) âš¡
| Model | Response Time | Streaming |
|-------|--------------|-----------|
| `vortex-llama-guard-4-12b` | 189ms | âœ… |
| `vortex-gemma2-9b-it` | 198ms | âœ… |
| `vortex-llama3-70b-8192` | 220ms | âœ… |
| `vortex-llama-4-scout-17b-16e` | 355ms | âœ… |
| `vortex-llama-3.3-70b-versatile` | 373ms | âœ… |
| `vortex-allam-2-7b` | 374ms | âœ… |
| `vortex-llama3-8b-8192` | 377ms | âœ… |
| `vortex-llama-prompt-guard-2-86m` | 380ms | âŒ |
| `vortex-llama-prompt-guard-2-22m` | 392ms | âŒ |
| `vortex-gpt-oss-20b` | 393ms | âœ… |

### Fast (<1s) ğŸš€
| Model | Response Time | Streaming |
|-------|--------------|-----------|
| `vortex-gpt-oss-120b` | 460ms | âœ… |
| `vortex-qwen3-32b` | 470ms | âœ… |
| `vortex-llama-3.1-8b-instant` | 620ms | âœ… |
| `vortex-compound-beta-mini` | 626ms | âœ… |
| `vortex-deepseek-r1-distill-llama-70b` | 735ms | âœ… |
| `vortex-compound-beta` | 995ms | âœ… |

### Standard (1-2s) 
| Model | Response Time | Streaming |
|-------|--------------|-----------|
| `vortex-llama-4-maverick-17b` | 1494ms | âœ… |
| `vortex-kimi-k2-instruct` | 1555ms | âœ… |

## ğŸŒŸ Special Models

### ğŸ›¡ï¸ Safety/Guard Models
- `vortex-llama-guard-4-12b` - Content moderation (189ms!)
- `vortex-llama-prompt-guard-2-22m` - Prompt safety (392ms)
- `vortex-llama-prompt-guard-2-86m` - Enhanced prompt safety (380ms)

### ğŸŒ Multilingual
- `vortex-allam-2-7b` - Arabic language model (374ms)
- `vortex-kimi-k2-instruct` - Chinese/English (1555ms)

### ğŸ’ª Large Models
- `vortex-gpt-oss-120b` - 120B parameters (460ms!)
- `vortex-llama3-70b-8192` - 70B parameters (220ms!)
- `vortex-llama-3.3-70b-versatile` - Latest Llama 3.3 70B (373ms)

### ğŸ§ª Experimental
- `vortex-compound-beta` - Experimental model (995ms)
- `vortex-compound-beta-mini` - Smaller experimental (626ms)

## ğŸ“Š Streaming Support

**16 out of 18 models support streaming** (89%)

Models without streaming:
- `vortex-llama-prompt-guard-2-22m`
- `vortex-llama-prompt-guard-2-86m`

## ğŸ‘ï¸ Vision Support

**Currently**: No models support vision through this endpoint
- All models tested with image input
- None successfully processed images
- Text-only capabilities confirmed

## ğŸš€ Usage Examples

### Basic Chat
```bash
curl https://ahamai-api.officialprakashkrsingh.workers.dev/v1/chat/completions \
  -H "Authorization: Bearer ahamaibyprakash25" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vortex-gemma2-9b-it",
    "messages": [{"role": "user", "content": "Hello!"}],
    "stream": false
  }'
```

### Streaming Response
```bash
curl https://ahamai-api.officialprakashkrsingh.workers.dev/v1/chat/completions \
  -H "Authorization: Bearer ahamaibyprakash25" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vortex-llama3-70b-8192",
    "messages": [{"role": "user", "content": "Tell me a story"}],
    "stream": true
  }'
```

### Content Moderation
```bash
curl https://ahamai-api.officialprakashkrsingh.workers.dev/v1/chat/completions \
  -H "Authorization: Bearer ahamaibyprakash25" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vortex-llama-guard-4-12b",
    "messages": [{"role": "user", "content": "Check if this text is safe"}]
  }'
```

### Arabic Language
```bash
curl https://ahamai-api.officialprakashkrsingh.workers.dev/v1/chat/completions \
  -H "Authorization: Bearer ahamaibyprakash25" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "vortex-allam-2-7b",
    "messages": [{"role": "user", "content": "Ù…Ø±Ø­Ø¨Ø§"}]
  }'
```

## ğŸ’¡ Recommendations

### For Speed
1. **vortex-llama-guard-4-12b** (189ms) - Fastest overall
2. **vortex-gemma2-9b-it** (198ms) - Great balance
3. **vortex-llama3-70b-8192** (220ms) - Fast 70B model

### For Quality
1. **vortex-gpt-oss-120b** - Largest model (120B)
2. **vortex-llama-3.3-70b-versatile** - Latest Llama
3. **vortex-deepseek-r1-distill-llama-70b** - Reasoning

### For Specific Use Cases
- **Arabic**: vortex-allam-2-7b
- **Safety**: vortex-llama-guard-4-12b
- **Chinese**: vortex-kimi-k2-instruct
- **General Chat**: vortex-gemma2-9b-it

## ğŸ”§ Technical Details

### Endpoint Configuration
```javascript
// No authentication required
const VORTEX_URL = "http://141.11.210.85:3000/api/openai/v1/chat/completions";

// All models use the same endpoint
"vortex-model-name": VORTEX_URL
```

### Model Naming Convention
All VORTEX models are prefixed with `vortex-` to distinguish them:
- Original: `llama3-70b-8192`
- Exposed as: `vortex-llama3-70b-8192`

## ğŸ“ˆ Statistics

- **Total Models**: 18
- **Working**: 18 (100%)
- **Average Response Time**: 541ms
- **Fastest**: 189ms (vortex-llama-guard-4-12b)
- **Slowest**: 1555ms (vortex-kimi-k2-instruct)
- **Streaming Support**: 89% (16/18)
- **Vision Support**: 0% (0/18)

## ğŸ‰ Summary

VORTEX models are a fantastic addition:
- âœ… **100% reliability** - All models working
- âš¡ **Extremely fast** - Most under 400ms
- ğŸ†“ **No authentication** - Works out of the box
- ğŸŒŠ **Streaming support** - 89% of models
- ğŸŒ **Diverse options** - From 7B to 120B parameters

These models significantly expand the available options and provide some of the fastest response times in the entire API!

---
*Last Updated: August 23, 2025*